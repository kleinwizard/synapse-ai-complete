version: '3.8'

services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    restart: unless-stopped
    environment:
      POSTGRES_DB: synapse_ai
      POSTGRES_USER: synapse_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
      - db_logs:/var/log/postgresql
    ports:
      - "127.0.0.1:5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U synapse_user -d synapse_ai"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - synapse-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=database,environment=production"
    command: |
      postgres
      -c logging_collector=on
      -c log_destination=stderr
      -c log_statement=all
      -c log_duration=on
      -c log_min_duration_statement=1000
      -c log_checkpoints=on
      -c log_connections=on
      -c log_disconnections=on
      -c log_lock_waits=on
      -c log_temp_files=0
      -c log_autovacuum_min_duration=0
      -c log_error_verbosity=verbose
      -c log_hostname=on
      -c log_line_prefix='%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

  # Backend API
  api:
    build: 
      context: ./backend
      dockerfile: Dockerfile.prod
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://synapse_user:${DB_PASSWORD}@db:5432/synapse_ai
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY}
      STRIPE_PUBLISHABLE_KEY: ${STRIPE_PUBLISHABLE_KEY}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET}
      SENDGRID_API_KEY: ${SENDGRID_API_KEY}
      CORS_ORIGIN_URL: https://${DOMAIN}
      ENVIRONMENT: production
      USE_LOCAL_OLLAMA: false
      # Enhanced logging configuration
      LOG_LEVEL: INFO
      LOG_FORMAT: json
      LOG_FILE: /app/logs/synapse_ai.log
      PYTHONUNBUFFERED: 1
    ports:
      - "127.0.0.1:8000:8000"
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - api_logs:/app/logs
      - upload_files:/app/uploads
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - synapse-network
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "10"
        labels: "service=backend,environment=production,component=api"

  # Frontend (build and serve)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
      args:
        VITE_API_BASE_URL: https://${DOMAIN}/api
        VITE_ENABLE_LOGGING: "true"
        VITE_LOG_LEVEL: INFO
        VITE_LOGGING_ENDPOINT: https://${DOMAIN}/api/logs
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:80"
    networks:
      - synapse-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=frontend,environment=production,component=web"

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - /etc/letsencrypt:/etc/letsencrypt:ro
      - nginx_logs:/var/log/nginx
      - ./nginx/html:/var/www/html:ro
    depends_on:
      - api
      - frontend
    networks:
      - synapse-network
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
        labels: "service=nginx,environment=production,component=proxy"
    environment:
      - NGINX_ENVSUBST_TEMPLATE_DIR=/etc/nginx/templates
      - NGINX_ENVSUBST_OUTPUT_DIR=/etc/nginx/conf.d

  # Redis for caching and sessions
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: |
      redis-server 
      --appendonly yes 
      --maxmemory 256mb 
      --maxmemory-policy allkeys-lru 
      --requirepass ${REDIS_PASSWORD}
      --loglevel notice
      --logfile ""
      --syslog-enabled yes
      --syslog-ident redis-synapse
    volumes:
      - redis_data:/data
      - redis_logs:/var/log/redis
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - synapse-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=redis,environment=production,component=cache"

  # Log aggregation and monitoring (optional)
  # Uncomment and configure for centralized logging
  # logstash:
  #   image: docker.elastic.co/logstash/logstash:8.11.0
  #   restart: unless-stopped
  #   volumes:
  #     - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
  #     - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
  #     - nginx_logs:/var/log/nginx:ro
  #     - api_logs:/var/log/api:ro
  #     - db_logs:/var/log/db:ro
  #   ports:
  #     - "127.0.0.1:5044:5044"
  #   environment:
  #     - XPACK_MONITORING_ENABLED=false
  #   networks:
  #     - synapse-network
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "100m"
  #       max-file: "3"
  #       labels: "service=logstash,environment=production,component=logging"

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  api_logs:
    driver: local
  upload_files:
    driver: local
  nginx_logs:
    driver: local
  db_logs:
    driver: local
  redis_logs:
    driver: local

networks:
  synapse-network:
    driver: bridge